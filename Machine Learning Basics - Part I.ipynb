{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37827b96",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c3032e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data for the polynomial degree demo.\n",
    "n_polynomial_degrees = 20\n",
    "num_samples = 10\n",
    "\n",
    "\n",
    "class PolynomialDegreeDemo:\n",
    "    x = np.linspace(-10, 10, num_samples) + np.random.normal(0, 1, num_samples)\n",
    "    y = np.linspace(0, 1, num_samples) + np.random.normal(0, 0.1, num_samples)\n",
    "    xp = np.linspace(-15, 15, 200)\n",
    "    yp = []\n",
    "\n",
    "\n",
    "for i in range(n_polynomial_degrees):\n",
    "    degree = i + 1\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', np.RankWarning)\n",
    "        x = PolynomialDegreeDemo.x\n",
    "        y = PolynomialDegreeDemo.y\n",
    "        xp = PolynomialDegreeDemo.xp\n",
    "        p = np.poly1d(np.polyfit(x, y, degree))\n",
    "        PolynomialDegreeDemo.yp.append(p(xp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d5fbb4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data for the bias and variance demos.\n",
    "class BiasAndVarianceDemo:\n",
    "    alphas = np.geomspace(2**3 * 1e-4, 2**13 * 1e-4, num=6, endpoint=True)\n",
    "    xp = np.linspace(0.05, 0.95, 1000)\n",
    "    ypss = []\n",
    "    yps_mean = []\n",
    "    alpha_to_index = {}\n",
    "    \n",
    "    def func(x):\n",
    "        return np.sin(2 * np.pi * x)\n",
    "\n",
    "\n",
    "size = 25    \n",
    "for i, alpha in enumerate(BiasAndVarianceDemo.alphas):\n",
    "    xp = BiasAndVarianceDemo.xp\n",
    "    yps = []\n",
    "    for _ in range(20):\n",
    "        x = np.random.uniform(-0.1, 1.1, size)\n",
    "        y = BiasAndVarianceDemo.func(x) + np.random.normal(0, 0.1, size)\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(0.5, (1e-1, 1e1))\n",
    "        model = GaussianProcessRegressor(kernel=kernel, alpha=alpha, n_restarts_optimizer=24)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model = model.fit(x[:, np.newaxis], y)\n",
    "        yp = model.predict(xp[:, np.newaxis])\n",
    "        yps.append(yp)\n",
    "    BiasAndVarianceDemo.ypss.append(yps)\n",
    "    BiasAndVarianceDemo.yps_mean.append(np.mean(yps, axis=0))\n",
    "    BiasAndVarianceDemo.alpha_to_index[alpha] = i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa23237e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def _polynomial_degree(degree):\n",
    "    x = PolynomialDegreeDemo.x\n",
    "    y = PolynomialDegreeDemo.y\n",
    "    xp = PolynomialDegreeDemo.xp\n",
    "    yp = PolynomialDegreeDemo.yp[degree - 1]\n",
    "\n",
    "    plt.plot(xp, yp)\n",
    "    plt.scatter(x, y)\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "def _bias(alpha):\n",
    "    i = BiasAndVarianceDemo.alpha_to_index[alpha]\n",
    "    xp = BiasAndVarianceDemo.xp\n",
    "    yp = BiasAndVarianceDemo.yps_mean[i]\n",
    "\n",
    "    plt.plot(xp, yp)\n",
    "    plt.plot(xp, BiasAndVarianceDemo.func(xp), color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "def _variance(alpha):\n",
    "    i = BiasAndVarianceDemo.alpha_to_index[alpha]\n",
    "    xp = BiasAndVarianceDemo.xp\n",
    "    yps = BiasAndVarianceDemo.ypss[i]\n",
    "\n",
    "    for yp in yps:\n",
    "        plt.plot(xp, yp, linewidth=1, alpha=0.5)\n",
    "    plt.plot(xp, BiasAndVarianceDemo.func(xp), color='k', linestyle='--', linewidth=1)\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class Demos:\n",
    "    def polynomial_degree(self):\n",
    "        interact(_polynomial_degree, degree=widgets.IntSlider(min=1,max=n_polynomial_degrees, step=1, value=1));\n",
    "        \n",
    "    def bias(self):\n",
    "        interact(_bias, alpha=widgets.SelectionSlider(options=BiasAndVarianceDemo.alphas));\n",
    "\n",
    "    def variance(self):\n",
    "        interact(_variance, alpha=widgets.SelectionSlider(options=BiasAndVarianceDemo.alphas));\n",
    "        \n",
    "demos = Demos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299126ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Basics\n",
    "\n",
    "- *Hyperparameters and Validation Sets*\n",
    "\n",
    "- *Estimators, Bias & Variance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02759d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters and Validation Sets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da438dd5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Notes\n",
    "> - In this section:\n",
    ">  - What are hyperparameters and validations sets\n",
    ">  - How do they realate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c942bc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7326c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Settings that control a model's behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e5a73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Not adapted by the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838dd26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There could be a second learning algorithm to learn the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201c597",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Polynomial regression has a single **capacity** hyperparameter - the degree of the polynomial $D$:\n",
    "\n",
    "$$\\hat{y} = b + \\sum_{i=1}^{D}{w_{i} x^{i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2205b641",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027cc29633594c51aed9a5628ac81b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='degree', max=20, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demos.polynomial_degree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69ab67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The weight decay strenght $\\lambda$ is another example of a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2c754",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Notes:\n",
    "> - An example of polynomial regression.\n",
    "> - The function that generates the data is a line equation with added gaussian noise.\n",
    "> ---\n",
    "> - It is often innapropriate to learn the hyperparameters from the training dataset.\n",
    "> - What would happen in the example above if the degree was learnt from the training dataset?\n",
    "> - If learned on the training set, the hyperparameters would change such that the capacity is maximised and hence \n",
    ">   overfit.\n",
    "> ---\n",
    "> - As seen above, a high degree polynomial fits the training data perfectly, but a low degree polynomial matches \n",
    ">   the true data-generating distribution much better.\n",
    "> ---\n",
    "> - How are hyperparameters learned then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0487929",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d46e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Typically an **80/20 split** of the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e3fe0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training set used to **learn model parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524632b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Validation set used to **estimate generalisation error** during or after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711449a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![assets/capacity_vs_error.png](assets/capacity_vs_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def0a5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <font color='red'>**Problematic if the dataset is small.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2edb2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Notes:\n",
    ">  - Split the whole dataset into two disjoint datasets.\n",
    "> \n",
    ">  - Validation set helps with tuning the hyperparameters!\n",
    ">\n",
    ">  - If the dataset is small, the test set becomes tiny; this implies statistical uncertainty aroung the estimated\n",
    ">    generalisation error, making it harder to compare algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140c81e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdbf29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\mathbb{D} = \\left\\{ z_i \\right\\}_{i=1}^{20} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039f231",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![assets/cv0.png](assets/cv0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1e050",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example of 5-fold cross-validation:\n",
    "\n",
    "![assets/cv.gif](assets/cv.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7022cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimators, Bias & Variance\n",
    "\n",
    "*Foundational concepts useful for formalizing generalization, underfitting and overfitting.*\n",
    "\n",
    "### Point Estimation\n",
    "\n",
    "\n",
    "### Function Estimation\n",
    "\n",
    "### Bias\n",
    "$$ bias\\left( \\hat{\\boldsymbol{\\theta}}_m \\right) = \\mathbb{E} \\left( \\hat{\\boldsymbol{\\theta}}_m \\right) - \\boldsymbol{\\theta} $$\n",
    "\n",
    "### Variance\n",
    "$$ Var\\left( \\hat{\\boldsymbol{\\theta}} \\right) $$\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f0ec2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Illustration of the dependence of bias and variance on model complexity, governed by a regularization parameter λ, using the sinusoidal data set from Chapter 1. There are L = 100 data sets, each having N = 25\n",
    "data points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is\n",
    "M = 25 including the bias parameter. The left column shows the result of fitting the model to the data sets for\n",
    "various values of ln λ (for clarity, only 20 of the 100 fits are shown). The right column shows the corresponding\n",
    "average of the 100 fits (red) along with the sinusoidal function from which the data sets were generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8348429b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f678d4b8dedc446780daa10949f9fda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(description='alpha', options=(0.0008, 0.0032000000000000023, 0.012799999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demos.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcde886c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0073af97f84bf6bc505fddddcc9d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(description='alpha', options=(0.0008, 0.0032000000000000023, 0.012799999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demos.bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e965c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![assets/capacity_vs_bias_variance.png](assets/capacity_vs_bias_variance.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
